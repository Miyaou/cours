---
title:          "PRSTA: TD 5"
date:           2021-10-29 14:30
categories:     [Image S9, PRSTA]
tags:           [Image, S9, PRSTA]
math: true
---
Lien de la [note Hackmd](https://hackmd.io/@lemasymasa/SJQHnvYIY)

# Feuille 3 Exercice 4

La variable aleatoire $X$ suit une loi de Pareto de parametre $\alpha$.A l‚Äôaide du theoreme de Wilks, ecrire la zone de rejet du test $H_0 : \alpha = 2$ contre $H_1 : \alpha \gt 2$.

<details markdown="1"><summary>Solution</summary>

<div class="alert alert-danger" role="alert" markdown="1">
Nous n'avons pas de valeur pour $H_1$, mais $\alpha\gt 2$. Nous allons donc le remplacer par l'EMV.
</div>

Pour la loi de Pareto de parametre $\alpha\gt 0$ dont la densite est donnee par

$$
f(x,\alpha) = \alpha x^{-\alpha-1}
$$

pour $x\gt 1$.

Determinons l'EMV.

On a:

$$
L(x,\alpha) = \alpha^{n}\prod_{i=1}^nx_i^{-\alpha-1}
$$

d'ou

$$
\log L(x,\alpha) = n\log \alpha + \sum_{i=1}^n(-\alpha-1)\log(x_i)
$$

et

$$
\frac{\partial\log L}{\partial\alpha}(x,\alpha) = \frac{n}{\alpha}-\sum_{i=1}^n\log(x_i)
$$

Ainsi

$$
\frac{\partial\log L}{\partial \alpha}(x,\alpha) =0
$$

equivaut a

$$
\frac{n}{\alpha}-\sum_{i=1}^n\log(x_i) = 0
$$

Nous obtenons la solution $\hat\alpha = \frac{n}{\sum_{i=1}^n\log (x_i)}$

Reste a verifier la condition du second ordre:

$$
\frac{\partial^2\log L}{\partial\alpha^2} = -\frac{n}{\alpha^2}\lt 0
$$

<div class="alert alert-success" role="alert" markdown="1">
Par consequent, $\hat\alpha = \frac{n}{\sum_{i=1}^n\log(x_i)}$ est bien l'EMV
</div>

$$
\begin{aligned}
T&= \frac{L(X_1,\dots,X_n,\hat\alpha)}{L(X_1,\dots,X_n,2)}\\
&= \frac{\prod_{i=1}^n(\frac{n}{\sum_{j=1}^n\ln(X_j)})X_i^{-(\frac{n}{\Sigma \ln(X_i)+1})}}{\prod_{i=1}^n2X_i^{-3}}\\
&= \biggr(\frac{n}{2\Sigma\ln(X_j)}\biggr)^n\prod_{i=1}^nX_i^{-\frac{n}{\Sigma\ln(X_i)+2}}
\end{aligned}
$$

$$
\begin{aligned}
R_n&= 2\ln(T)\\
&= 2n\ln(\frac{n}{2S})+\sum_{i=1}^n(2-\frac{n}{S})\ln(X_i)
\end{aligned}\\
\color{red}{S:=\sum_{j=1}^n\ln(X_j)}\\
\begin{aligned}
Rn &= 2n\ln(\frac{n}{2S})+(2-\frac{n}{S})S\\
&= \boxed{2n\ln(\frac{n}{2S})+2S-n}
\end{aligned}
$$

Asymptotiquement, $R_n$ suit asymptotiquement une loi de $\chi^2$ a $n$ degre de liberte.

La zone de rejet est:

$$
\{R_n\gt\chi^2_{\color{red}{1-\alpha}}\}
$$

ou $\chi^2_{1-\alpha}$ designe le quantile de niveau $1-\alpha$

</details>

# Feuille 3 Exercice 6

Considerons $n$ variables aleatoires independantes de densite:

$$
f(x,\theta) = \theta^2xe^{-\theta x}ùüô_{\mathbb R_+}(x)
$$

ou le parametre $\theta$ est strictement positif.

Nous disponsons de $n$ observations et voulons tester l'hypothese $H_0:\theta = \theta_0$ contre l'hypothese $H_1:\theta = \theta_1$ avec $\theta_0\lt \theta_1$

1. Justifier que $f(x,\theta)$ definit bien une densite pour tout $\theta\gt 0$
2. Calculer $E(X)$
3. Determiner la statistique de Neyman-Pearson que nous noterons $T_n$
4. En admettant que $\theta T_n$ suit une loi $\Gamma(2n,1)$, determiner une expression de $\alpha$ et $\beta$ en fonction du seuil du test
5. Determiner les courbes COR associes a ce test.

<details markdown="1"><summary>Solution</summary>

<div class="alert alert-info" role="alert" markdown="1">
On saute les 2 premieres questions car fait et refait
</div>

3.

$$
\begin{aligned}
T &= \frac{L(X_n,\dots,X_n,\theta_1)}{L(X_n,\dots,X_n,\theta_0)}\\
&= \frac{\prod_{i=1}^n\theta_1^2X_ie^{-\theta_1X_i}}{\prod_{i=1}^n\theta_0^2X_ie^{-\theta_0X_i}}\\
&= \biggr(\frac{\theta_1}{\theta_0}\biggr)^{2n}\times e^{\sum_{i=1}^n(\theta_0-\theta_1)}
\end{aligned}
$$

On passe au logarithme:

$$
\begin{aligned}
\ln T&= \underbrace{2n\log(\frac{\theta_1}{\theta_0})}_{\color{green}{a}}+\underbrace{(\theta_0-\theta_1)}_{\color{green}{b}}\sum_{i=1}^nX_i
\end{aligned}
$$

L'hypothese $H_0$ est rejetee lorsque:

$$
\begin{aligned}
T&\gt C_{\alpha}\\
\ln T&\gt\ln C_{\alpha}\\
a+b\sum_{i=1}^nX_i&\gt\ln (C_{\alpha})\\
\underbrace{\sum_{i=1}^n X_i}_{\color{red}{T_n}}&\lt \underbrace{\frac{\ln(C_{\alpha})-a}{b}}_{\color{red}{S_{\alpha}}}
\end{aligned}\\
\color{green}{\text{car } b = \theta_0-\theta_1\lt 0}
$$

Donc: 

$$
T_n\lt S_{\alpha}
$$

4.

$$
\begin{aligned}
\alpha &= P(\text{Rejeter } H_0\vert H_0\text{ vraie})\\
&= P(T_n\lt S_{\alpha}\vert \theta=\theta_0)
\end{aligned}
$$

Sous $H_0$, $\theta_0 T_n$ suit une loi $\Gamma(2n, 1)$

$$
\begin{aligned}
\alpha &= P(\theta_0T_n\lt\theta_0 S_{\alpha})\\
&= F_n(\theta_0S_{\alpha})
\end{aligned}
$$

Ou $F_n$ designe la *fonction de repartition* de la loi $\Gamma(2n,1)$.

Exprimons $S_{\alpha}$ en fonction de $\alpha$:

<div class="alert alert-danger" role="alert" markdown="1">

$$
\boxed{S_{\alpha}=\frac{F_n^{-1}(\alpha)}{\theta_0}}
$$

</div>

$$
\begin{aligned}
\beta&= P(\text{Rejeter }H\vert H\text{ vraie})\\
&= P(T_n\ge S_{\alpha}\vert \theta=\theta_1)\\
&= P(\theta_1T_n\ge\theta_1S_{\alpha}\vert\theta=\theta_1)
\end{aligned}
$$

Or sous $H_1$: $\theta T_n\sim\Gamma(2n,1)$

Donc:

<div class="alert alert-danger" role="alert" markdown="1">

$$
\boxed{\begin{aligned}\beta&=1-F_n(\theta,S_{\alpha})\\
&=1-F_n(\frac{\theta}{\theta_0}F_n^{-1}(\alpha))\end{aligned}}
$$

</div>

En python:

```python
scipy.stats.gamma.cdf(2 * scipy.stats.gamma.ppf(0.05, 20, scale=1), 20, scale = 1)
```
```
0.9184...
```
```python
scipy.stats.gamma.cdf(2 * scipy.stats.gamma.ppf(0.05, 50, scale=1), 50, scale = 1)
```
```
0.999702...
```
```python
scipy.stats.gamma.cdf(2 * scipy.stats.gamma.ppf(0.01, 10, scale=1), 10, scale = 1)
```
```
0.316165...
```
```python
scipy.stats.gamma.cdf(2 * scipy.stats.gamma.ppf(0.001, 100, scale=1), 100, scale = 1)
```
```
0.9999523...
```

On nome $\Pi$ la probabilite de detection:

$$
\Pi = 1-\beta\\
\boxed{\Pi = F_n(\frac{\theta_1}{\theta_0}F_n^{-1}(\alpha))}
$$

</details>

# Feuille 4 Exercice 4

Considerons $n$ variables aleatoires independantes $X_i$ suivant la loi de densite:

$$
f(x,\theta) = \frac{3}{\theta} x^2e^{-\frac{x^3}{\theta}}ùüô_{\mathbb R_+(x)}
$$

avec $\theta\gt 0$ ou $ùüô_{\mathbb R_+}$ designe la fonction indicatrice de $\mathbb R_+$

Nous souhaitons tester l'hypothese $H_0:\theta = \theta_0$ contre $H_1:\theta = \theta_1$ avec $\theta_0\lt \theta_1$ a l'aide d'observations $x_i$ issues de l'echantillon precedent

1. 
    - (a) Justifier que, pour tout $\theta\gt0$, $f(\cdot,\theta)$ definit bien une densite sur $\mathbb R$
    - (b) Determiner l'EMV $\hat\theta$
2. Determiner la statistique du test de Neyman-Pearson et indiquer la region critique associe a ce test.
3. Verifier que la variable aleatoire $Y_i=\frac{2}{\theta}X_i^3$ suit une loin $\chi^2$ a deux degres de liberte
4. En deduire le seuil du test de Neyman-Pearson en fonction du risque de premiere espece $\alpha$
5. Determiner la puissance du test en fonction du test et de $\theta_1$
6. Determiner les courbes COR associees a ce test
7. 
    - (a) *Application numerique $1$ : $\alpha = 5\%, \theta_0 = 1, \theta_1=2$ et $n=15$*
    - (b) *Application numerique $1$ : $\alpha = 5\%, \theta_0 = 1, \theta_1=5$ et $n=30$*
    - (c) *Application numerique $1$ : $\alpha = 5\%, \theta_0 = 1, \theta_1=2$ et $n=10$*
    - (d) *Application numerique $1$ : $\alpha = 5\%, \theta_0 = 1, \theta_1=5$ et $n=30$*

<details markdown="1"><summary>Solution</summary>

3.

On pose $\phi(y)=\frac{2}{\theta}y^3$.

Ainsi:

$$
\phi^{-1}(y) = \sqrt[3]{\frac{\theta y}{2}}
$$

Elle est derivable car elle est polynomiale et est bijective car elle est strictement croissante.

$$
\begin{aligned}
f_Y(y)&=\frac{1}{(\frac{6}{\theta}(\sqrt[3]{\frac{\theta y}{2}})^2)}\times f(\sqrt[3]{\frac{\theta y}{2}})\\
&= \frac{1}{\frac{6}{\theta}(\sqrt[3]{\frac{\theta y}{2}})^2}\times \frac{3}{\theta}(\sqrt[3]{\frac{\theta y}{2}})^2\times e^{-(\frac{(\sqrt[3]{\frac{\theta y}{2}})^3}{\theta})}\\
&= \frac{1}{2}\times e^{-\frac{y}{2}}
\end{aligned}
$$

On peut en deduire que $Y$ suit une loi $\chi^2(2)$

4.

$$
\color{green}{\boxed{T=\sum_{i=1}^nX_i^3}}
$$

$$
\color{green}{Y_{i} = \frac{2}{\theta}X_i^3\sim X^2(2)}
$$

$$
\Rightarrow \frac{2}{\theta} T\sim \chi^2(2n)
$$

$$
\begin{aligned}
\alpha &=P(\text{Rejeter }H_0\vert H_0\text{ vraie})\\
&= P(T\gt S_{\alpha}\vert\theta = \theta_0)\\
&= P(\frac{2}{\theta_0}T\gt \frac{2}{\theta_0}S_{\alpha}\vert \theta=\theta_0)
\end{aligned}
$$

Sous $(H_0)$, $\color{red}{\frac{2}{\theta_0}T\sim\chi^2(2n)}$

$\color{green}{F_n \text{ est la fonction de repartition }\chi^2(2n)}$

$$
\alpha = P(W\gt \frac{2}{\theta_0}S_{\alpha})
$$

<div class="alert alert-danger" role="alert" markdown="1">

$$
\alpha = 1 -F_n(\frac{2}{\theta_0}S_{\alpha})
$$

</div>

$\color{red}{Donc}$

$$
1-\alpha = F_n(\frac{2}{\theta_0}S_{\alpha})
$$

<div class="alert alert-danger" role="alert" markdown="1">

$$
S_{\alpha} = \frac{\theta_0}{2}F_n^{-1}(1-\alpha)
$$

</div>

$$
\begin{aligned}
\color{red}{\beta} &= P(\text{Rejeter }H_1\vert H_1\text{vraie})\\
&= P(T\le S_{\alpha}\vert \theta=\theta_1)\\
&= P(\frac{2}{\theta_1}T\le \frac{2}{\theta_1}S_{\alpha}\vert \theta = \theta_1)
\end{aligned}
$$

$$
w_1 = \frac{2}{\theta_1}T\sim \chi^2(2n)
$$

<div class="alert alert-danger" role="alert" markdown="1">

$$
\beta = F_n(\frac{2}{\theta_1}S_{\alpha})
$$


$$
\color{green}{\beta = F_n\biggr(\frac{\theta_0}{\theta_1}F_n^{-1}(1-\alpha)\biggr)}
$$

</div>

Passons aux applications numeriques:

```python
scipy.stats.chi2.cdf(0.5 * scipy.stats.ppf(0.95, 30), 30)
```

```
0.14185880202947254
```

```python
scipy.stats.chi2.cdf(0.2 * scipy.stats.ppf(0.95, 60), 60)
```

```
1.6239064341119149e-09
```

```python
scipy.stats.chi2.cdf(0.5 * scipy.stats.ppf(0.99, 20), 20)
```

```
0.46403880816957155
```

```python
scipy.stats.chi2.cdf(0.2 * scipy.stats.ppf(0.99, 20), 20)
```

```
1.87204631776198e-08
```

```python
scipy.stats.chi2.cdf(1.0001 * scipy.stats.ppf(0.99, 20), 20)
```

```
0.9900104784496678
```

</details>
